\chapter{Introduction}

\selectlanguage{english}

\section{PMCTrack: delivering hardware performance monitoring
counter support}\label{pmctrack-management-of-the-hardware-performance-monitoring-counters}

Most modern complex computing systems are equipped with hardware
Performance Monitoring Counters (PMCs) that enable users to collect
application's performance metrics, such as the number of instructions
per cycle (IPC) or the Last-Level Cache (LLC) miss rate. These
PMC-related metrics aid in identifying possible performance bottlenecks,
thus providing valuable hints to programmers and computer architects.
Notably, direct access to PMCs is typically restricted to code running
at the OS privilege level. As such, a kernel-level tool, implemented in
the OS itself or as a driver, is usually in charge of providing
userspace tools with a high-level interface enabling to access
performance counters \cite{perfevents,perfmon2,oprofile}.

Previous work has demonstrated that the OS scheduler can also benefit
from PMC data making it possible to perform sophisticated and effective
runtime optimizations on multicore systems
\cite{observations,cache-aware-asplos,merkel-eurosys10,akula,intel-amp,camp,petrucci-tecs15,acfs}.
While public-domain tools to access PMCs make it possible to monitor
application performance from userspace, they do not provide an
architecture-independent API empowering the OS itself to leverage PMC
information to drive scheduling decisions. As a result, many researchers
turned to architecture-specific \textit{ad-hoc} code to access
performance counters when implementing different scheduling schemes
\cite{observations,intel-amp,camp,acfs}. This approach, however, leads
the scheduler implementation to be tied to a certain architecture or
processor model, and at the same time, forces developers to deal with
(or even write themselves) the associated low-level routines to access
PMCs on each architecture targeted by the scheduler. To avoid facing
these issues, other researchers resorted to limited and simplistic
userspace scheduling prototypes
\cite{cache-aware-asplos,akula,petrucci-tecs15} that rely on existing
userspace-oriented PMC tools.

In order to overcome these limitations, it was proposed the development
of PMCTrack, a hardware-counter management tool for the Linux kernel. This tool was specifically designed to make it possible for the OS to use the counters for its internal tasks, such as process scheduling. This tool was originally
developed as a project carried out by students of the Computer Science School at the Complutense University of Madrid, back in 2012 \cite{MSDTFG12}. Today, several members of the Architecture and Technology on Computing Systems Research Group (ArTeCS) of the Complutense University take on the development and support of PMCTrack. In fact, simultaneously to the development of our Degree Thesis, new
features have been added to this tool beyond the extensions proposed in
the DT.

PMCTrack's novelty lies in the \textit{monitoring module} abstraction,
an architecture-specific extension responsible for providing any OS
scheduling algorithm that leverages PMC data with the performance
metrics it requires to function. This abstraction makes it possible to
implement architecture-independent OS scheduling algorithms.
Specifically, ensuring that a thread scheduler works on a new processor
model or architecture boils down to developing the associated
platform-specific monitoring module in a loadable kernel module. More
importantly, because PMCTrack offers an architecture-independent
interface to easily configure events and gather PMC data, the monitoring
module developer does not have to deal with the platform-specific
low-level code to access PMCs on a given architecture, which greatly
simplifies the implementation.

Despite being a tool specifically designed to aid the OS scheduler,
PMCTrack is also equipped with a set of command line tools and userspace
components. These userspace tools assist OS-scheduler designers during
the entire development life cycle, by complementing the existing
kernel-level debugging tools with PMC-related offline analysis and
tracing support. Moreover, due to the flexibility of PMCTrack's
monitoring modules, any kind of metric provided by modern hardware but
not modeled directly via performance counters, such as power consumption
or an application's cache footprint, can be also exposed to the OS
scheduler or to the user applications as PMCTrack's
\textit{virtual counters}.

\section{Alternatives to PMCTrack}\label{alternatives-to-pmctrack}

Several tools have been created for the Linux kernel in the last few
years \cite{oprofile,perfmon2,perf,papi,likwid,schedmon}, enabling to
hide the diversity of the various hardware interfaces to end users and
providing them with convenient access to PMCs from userspace. Overall,
these tools can be divided into two broad categories. The first group
encompasses tools such as OProfile \cite{oprofile}, perfmon2
\cite{perfmon2} or perf \cite{perf}, which expose performance counters
to the user via a reduced set of command-line tools. These tools do not
require to modify the source code of the application being monitored;
instead they act as external processes with the ability to receive PMC
data of another application. The second group of tools provides the user
with libraries to access counters from an application's source code,
thus constituting a fine-grained interface to PMCs. The libpfm
\cite{perfmon2} and PAPI \cite{papi} libraries follow this approach.

The perf \cite{perf} tool, which relies on the Linux kernel's
\emph{Perf Events} \cite{perfevents} subsystem, is possibly the most
comprehensive tool on the first category available at the time of this
writing. Although perf began as a PMC tool supporting a wide range of
processor architectures, it now empowers users with striking software
tracing capabilities enabling them to keep track of a process' system
calls or scheduler-related activity, or various network/file-related
operations executed on behalf of an application. Despite the potential
of perf and the other aforementioned tools, neither of them implement a
kernel-level architecture-agnostic mechanism enabling the OS scheduler
to leverage PMC data for its internal decisions. PMCTrack has been
specifically designed to fill this gap. As PMCTrack, perf has also the
capability to expose non-PMC hardware-related data exposed by modern
hardware to the user, such as the LLC occupancy.

Despite the potential of perf and other related tools, none of them
implements a mechanism that provides a kernel level
architecture-independent interface that allows the OS scheduler leverage
information from the PMCs for its internal decisions. This is the main
purpose of PMCTrack.

As PMCTrack, some performance monitoring tools require changes to the
Linux kernel to provide the desired functionality
\cite{perfmon2,kermon}. KerMon \cite{kermon} relies on a separate
scheduling class in the kernel to carry out low-level access to PMCs. To
collect PMC-data for an application via KerMon, the application must be
scheduled with the new scheduling class. This scheduling class is merely
a clone of the fair (CFS) class so it does not exploit PMC values to
make scheduling decisions. PMCTrack, on the other hand, makes it
possible for virtually any scheduling class created in the kernel to
gather performance metrics via an architecture-independent mechanism. To
this end, the kernel requires minimal changes, since as we show in the
next section, the vast majority of PMCTrack functionality is
encapsulated in a loadable kernel module.

\section{PMCTrack design}\label{pmctrack-design}

This section describes the internal architecture of PMCTrack, just as it
was before our DT started. We also present the different use modes originally supported by the tool.

\begin{figure}[tbp]
\centering
\input{Imagenes/Fuentes/architecture}
\caption{Architecture of PMCTrack}
\label{fig:arch}
\end{figure}

\subsection{Architecture}\label{architecture}

Figure \ref{fig:arch} depicts PMCTrack's internal architecture. The tool
consists of a set of user and kernel space components. At a high level,
the end user and the applications interact with PMCTrack using the
available command line tools. These components communicate with
PMCTrack's kernel module by means of a set of Linux \texttt{/proc}
entries exported by the module.

The kernel module implements the vast majority of PMCTrack's
functionality. To gather per-thread performance counter data, the module
needs to be fully-aware of thread scheduling events (e.g, context
switches, thread creation/termination). In addition to exposing
application's performance counter data to the userland tools, the module
implements a simple API to feed with per-thread monitoring data to any
scheduling policy (class) that requires performance-counter information
to function. Because both the core Linux Scheduler and scheduling
classes are implemented entirely in the kernel, making PMCTrack's kernel
module aware of these events and requests requires some minor
modifications to the Linux kernel itself. These modifications, referred
to as PMCTrack kernel API in Figure \ref{fig:arch}, comprise a set of
notifications issued from the core scheduler to the module.

To receive key notifications PMCTrack's kernel module implements the following interface:

\begin{minipage}{\linewidth}
  \begin{lstlisting}[frame=single,language=C]
  typedef struct pmc_ops{
    /* invoked when a new thread is created */
    void* (*pmcs_alloc_per_thread_data)(unsigned long,struct task_struct*);
    /* invoked when thread leaves the CPU */
    void (*pmcs_save_callback)(void*, int);
    /* invoked when thread enters the CPU */
    void (*pmcs_restore_callback)(void*, int);
    /* invoked every clock tick on a per-thread basis */
    void (*pmcs_tbs_tick)(void*, int);
    /* invoked when a process invokes exec() */
    void (*pmcs_exec_thread)(struct task_struct*);
    /* invoked when a thread exists the system */
    void (*pmcs_exit_thread)(struct task_struct*);
    /* invoked when a thread descriptor is freed up */
    void(*pmcs_free_per_thread_data)(struct task_struct*);
    /* invoked when the scheduler requests per-thread
          monitoring information  */
    int  (*pmcs_get_current_metric_value)(struct task_struct* task, int key, uint64_t* value);
  } pmc_ops_t;
  \end{lstlisting}
\end{minipage}


Most of these notifications get engaged only when PMCTrack's kernel
module is loaded and the user (or the scheduler itself) is using the
tool to monitor the performance of a specific application.

As shown in Figure \ref{fig:arch}, PMCTrack's kernel module consists of
various components. The architecture-independent core layer implements
\texttt{pmc\_ops\_t} interface and interacts with PMCTrack command line
tools via the Linux proc file system. The architecture-independent
component relies on a Performance Monitoring Unit Backend (PMU BE) to
carry out low-level access to performance counters, as well as for
translating user-provided counter configuration strings into internal
data structures for the platform in question. Currently there are
backends compatible with most modern processors from Intel and AMD.
Also, simultaneously to the development of our DT, the development of
two extra backends was carried out, one compatible with ARM's Cortex
processors of 32 and 64 bits, and another for Intel's Xeon Phi
coprocessor. PMCTrack's kernel module also includes a set of
platform-specific \textit{monitoring modules}. The primary purpose of a
monitoring module is to provide a scheduling algorithm implemented in
the kernel with high-level performance metrics.

%To provide support for PMCTrack to the Linux kernel implies the
%inclusion of two new files in the kernel itself, and the addition of
%less than 20 code lines to the kernel source. These changes can be
%easily applied to different kernel versions from version 2.6.38.

Augmenting a recent version of the Linux kernel (2.6.38 and above) to support PMCTrack entails including two new source files to the kernel tree (API for the module), and adding less than 20 extra lines of code. These changes can be easily applied to different kernel versions.

\subsection{PMCTrack Usage Models}\label{pmctrack-usage-models}

Before the beginning of our project, PMCTrack supported three
usage models: scheduler mode, time-based sampling and event-based
sampling.

\begin{figure}[tbp!]
\centering
\selectlanguage{english}
\input{Imagenes/Fuentes/mmon}
\caption{PMCTrack monitoring modules}
\label{fig:mmon}
\end{figure}

\subsubsection{Scheduler mode}\label{scheduler-mode}

This mode enables any scheduling algorithm in the kernel (i.e.,
scheduling class) to collect per-thread monitoring data, thus making it
possible to drive scheduling decisions based on tasks' memory behavior
or other microarchitectural properties. Turning on this mode for a
particular thread from the scheduler's code boils down to activating the
\texttt{prof\_enabled}
flag\footnote{This flag is added to Linux \texttt{task\_struct} when applying PMCTrack's kernel patch.}
in the thread's descriptor.

To ensure that the implementation of the scheduling algorithm that
benefits from this feature remains architecture independent, the
scheduler itself (implemented in the kernel) does not configure nor
deals with performance counters directly. Instead, one of PMCTrack's
\textit{monitoring modules} is in charge of feeding the scheduling
policy with the necessary high-level performance monitoring metrics,
such as a task's instruction per cycle ratio or its last-level cache
miss rate.

As shown in Figure \ref{fig:mmon}, PMCTrack may include several
monitoring modules compatible with a given platform. However, only one
can be enabled at a time: the one that provides the scheduler with the
PMC-related information it requires to function. In the event several
compatible monitoring modules are available, the system administrator
may tell the system which one to use by writing in the
\texttt{/proc/pmc/mmon\_manager} file. In a similar vein, the PMC
sampling period used by the monitoring module can be configured via the
\emph{/proc} file system.

The scheduler can communicate with the active monitoring module to
obtain per-thread data via the following function from PMCTrack's kernel
API:

\begin{lstlisting}[backgroundcolor=,basicstyle=\tt\footnotesize]
int pmcs_get_current_metric_value(struct task_struct*
   task, int metric_id, uint64_t* value);
\end{lstlisting}

For simplicity, each metric is assigned a numerical ID, known by the
scheduler and the monitoring module. To obtain up-to date metrics, the
aforementioned function may be invoked from the tick processing function
in the scheduler.

Monitoring modules make it possible for a scheduling policy relying on
performance counters to be seamlessly extended to new architectures or
processor models as long as the hardware enables to collect necessary
performance data. All that needs to be done is to build a monitoring
module or adapt an existing one to the platform in question. From the
programmer's standpoint, creating a monitoring module entails
implementing an interface very similar to \texttt{pmc\_ops\_t}.
Specifically, it consists of several callback functions enabling to
notify the module on activations/deactivations requested by the system
administrator, on threads' context switches, every time a thread
enters/exits the system, whenever the scheduler requests the value of a
per-thread PMC-related metric, etc. Nevertheless, the programmer
typically implements the subset of callbacks required to carry out the
necessary internal processing.

Creating new monitoring modules is a rather simple task for several
reasons. First, the programmer does not need to access
performance-counter registers directly. Instead, PMCTrack's kernel
module offers an architecture-independent API enabling the monitoring
module to specify the counter configuration via strings, to receive
performance counter samples periodically and to control event
multiplexing. Second, because the module gets notified when a new thread
is created or exits the system, the monitoring module may allocate
per-thread data to simplify any kind of thread-specific processing.
Third, because a monitoring module's code lives in PMCTrack's kernel
module, fixing most bugs does not require rebooting the system; instead
the kernel module can be unloaded/reloaded after applying the bug fix.

\subsubsection{Time-based sampling (TBS)}\label{time-based-sampling-tbs}

This feature allows the user to gather an application's performance data
from userspace at regular time intervals. The \texttt{pmctrack}
command-line tool, inspired in Solaris's \texttt{cputrack} program,
enables to use this feature. To illustrate how the tool works let us
consider the following example:

\begin{lstlisting}[language=bash,basicstyle=\tt\scriptsize]
$ pmctrack -T 1 -c pmc0,pmc3=0x2e,umask3=0x41 ./mcf06
nsample  event          pmc0          pmc3
      1   tick    1961001132        110634
      2   tick    1247853112          8323
      3   tick    1230836405          3859
      4   tick    1358134323        409386
      5   tick    1280630906       1199270
      6   tick    1231578609      15488307
...
\end{lstlisting}

In a system with a modern Intel processor, this command provides the
user with the number of instructions retired and last-level cache (LLC)
misses every second. Each sample is represented by a different row. The
sampling period can be specified in seconds via the -T option; fractions
of a second can be also specified (e.g, 0.3 for 300ms). If the user
includes the -A switch in the command line, \texttt{pmctrack} displays
the aggregate event counts for the application's entire execution
instead. At the end of the line, we specify the command to run the
associated application we wish to monitor (e.g: ./mcf06).

The -c option accepts as an argument a raw PMCTrack configuration string
which follows the internal event configuration format recognized by
PMCTrack kernel module. The raw format gives flexibility to experienced
users enabling them to decide the event-to-counter mapping or the actual
hex code that is written in the low-level PMC register fields exposed by
the kernel module. As we have seen, the raw string
\texttt{pmc0,pmc3=0x2e,umask3=0x41} would also enable to gather the
aforementioned event counts on most modern Intel processors. On
processors from the ARM Cortex Ax family, this event set can be
represented with the \texttt{pmc1=0x8,pmc2=0x17} raw string. If the user
does not know the hexadecimal codes that allow to associate an event
with a PMC, he should consult those codes in the architecture's manual
in question.

A striking feature of the time-based sampling usage model is the ability
to retrieve virtually any runtime metric gathered by the active
monitoring module such as energy consumption, the application's LLC
occupancy or any high-level metric determined by the module. To this
end, the monitoring module must explicitly expose the metrics as
\textit{virtual counters} using PMCTrack's API. Virtual counter values
can be retrieved using the -V option of the \texttt{pmctrack} program.

In case a specific processor model does not integrate enough PMCs to
monitor a given set of events at once, the user can turn to PMCTrack's
event-multiplexing feature. This boils down to specifying several event
sets by including multiple instances of the -c switch in the command
line. In this case, the various events sets will be collected in a
round-robin fashion and a new \textit{expid} field in the output will
indicate the event set a particular sample belongs to.

Note that in the event the active monitoring module is currently using
PMCs on behalf of a scheduling algorithm, the user is not allowed to
specify a PMC configuration string via the \texttt{pmctrack}'s -c
option. Nevertheless, for debugging purposes the pmctrack command may
still be used in this case (without the -c switch), to retrieve the
event counts associated with the configuration imposed by the monitoring
module.

To support the time-based sampling feature, the PMCTrack's kernel module
stores performance and virtual counter values in a per-application ring
buffer. The command-line tool retrieves samples from the kernel buffer
by reading from a /proc file that blocks the monitor process till new
samples are generated or the application terminates.

\subsubsection{Event-based sampling
(EBS)}\label{event-based-sampling-ebs}

Event-based Sampling (EBS) constitutes a variant of time-based sampling
wherein PMC values are gathered when a certain event count reaches a
certain threshold ($T$). To support EBS, PMCTrack's kernel module
exploits the interrupt-on-overflow feature present in most modern
Performance Monitoring Units (PMUs). At a high level, when EBS is turned
on, PMCTrack's kernel module initializes the associated performance
counter to $-T$; when this counter overflows the PMUs generates an
interrupt, upon which the kernel module reads all the counters.

To use the EBS feature from userspace, the \textit{ebs} flag must be
specified in \texttt{pmctrack}'s command line at the end of the
contiguration counters string. In doing so, a threshold value may be
also specified as in the following example:

\begin{lstlisting}[language=bash,basicstyle=\tt\scriptsize]
$ pmctrack -c pmc0,pmc3=0x2e,umask3=0x41,ebs0=500000000 ./mcf06
nsample  event          pmc0          pmc3
      1    ebs     500000087         10677
      2    ebs     500000002         22336
      3    ebs     500000004         17131
      4    ebs     500000007         12995
      5    ebs     500000014          9348
      6    ebs     500000010          5804
...
\end{lstlisting}

The \textit{pmc3} column displays the number of LLC misses every 500
million retired instructions. Note, however, that values in the
\textit{pmc0} column do not reflect exactly the target instruction
count. This has to do with the fact that, in modern processors, the PMU
interrupt is not served right after the counter overflows. Instead, due
to the out-of-order and speculative execution, several dozen
instructions or more may be executed within the period elapsed from
counter overflow until the application is actually interrupted. These
inaccuracies do not pose a big problem as long as coarse instruction
windows are used.

\section{Project goals}\label{project-goals}

The PMCTrack tool provides great features and empowers the OS scheduler
to take advantage of hardware counters to make optimizations at
run time. However, PMCTrack still has important limitations that cast a
shadow over its great possibilities, especially when it comes to monitoring application performance from user space.

First, its use turns out a bit complicated for inexperienced
users, since they have to have access to technical manuals of different
architectures to look up the appropriate hex codes for the events they want to monitor with the various PMCs.

Second, getting the specific events count on the PMCs on a timely basis does not provide the user the big picture on the evolution over time of the values for the different counters, let alone high level metrics made up by the
combination of two or more PMC values. This valuable information for the
user can be provided by means of charts. Nevertheless, this entails to go through various steps: (1) processing the data provided by the \texttt{pmctrack} command to
get the high level metrics values over time, and (2) using some helper utility, such as \emph{Gnuplot} to generate the final charts. Finally, another significant limitation of the tool is the fact that the \texttt{pmctrack} command (and the kernel module itself on which it relies) does not allow monitoring multithreaded applications from user space.

To address these and other related problems, this project pursues the following goals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Provide support for monitoring multithreaded applications
  from user space with PMCTrack.
\item
  Design and implement a graphical frontend for PMCTrack (referred to as
  PMCTrack-GUI) that enables real-time visualization of high-level performance metrics defined by the user.
\item
  Create \emph{libpmctrack}, a library that makes it possible to monitor code fragments of application programs by means of PMCs.
\end{enumerate}

\section{Work plan}\label{workplan}

To achieve the project goals, described above, the
project development consisted of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Divide work into individual tasks for the various members of the work group
\item
  Reading documentation about programming languages and other
  technologies used to carry out the development (Python, wxPython,
  matplotlib, \ldots). This step also entails getting familiar with PMCTrack internal architecture.
\item
  Making PMCTrack-GUI mockups to study different design alternatives and
  help reaching an agreement on the final design.
\item
  Augmenting PMCTrack with support for monitoring multithreaded applications from user space.
\item
  Implementation of PMCTrack-GUI.
\item
  Design and implementation of \emph{libpmctrack} and refactoring the
  code of the \texttt{pmctrack} command-line tool.
\item
  Developing the various case studies to test the different extensions incorporated into PMCTrack.
\end{enumerate}

It is worth noting that the order of these steps is merely
illustrative, since such steps were completed in a strictly sequential
way. Specifically, the development of the various components of PMCTrack required crafting a separate schedule, since we implemented diferent components simultaneously. In addition, changes in PMCTrack were made during our project due to maintainance issues (PMCTrack is a
tool in continous development), and new features not initially planned were implemented at the end, such as the inclusion of an SSH mode for PMCTrack-GUI. \selectlanguage{spanish}
